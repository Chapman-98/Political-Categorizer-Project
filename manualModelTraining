# Import necessary libraries
import newspaper
import nltk
import string
import csv
import openpyxl
import pandas as pd
from newspaper import Article
from bs4 import BeautifulSoup
from nltk.corpus import stopwords



# Global variables
VALID_CATEGORIES = [
    'politics', 'news', 'election', 'government', 'policy', 'congress', 'senate', 'house of representatives', 
    'legislation', 'political', 'voting', 'republic', 'democrat', 'republicanism', 'republican government',
    'government'
] # refined search

# Creates a set of stop words (generic words)
nltk.download('stopwords')
stopWords = set(stopwords.words('english'))



#############################################################################################################
#############################################################################################################
#############################################################################################################
####################################### SECTION 1: INDIVIDUAL URL ###########################################
#############################################################################################################
#############################################################################################################
#############################################################################################################


# A helper function that validates the given article
def validateArticle(article):
    # Validation #2: Check if the article is not a video article by checking the text size since video articles have more text
    if (len(article.text.split()) < 100):  
        print("Video articles are not acceptable!")
        return False

    # Validation #3: Check if the article is in English
    if article.meta_lang != 'en':
        print("Article not in English! Please provide an English article.")
        return False

    # Validation #4: Check if the article category is valid
    # Retrieves the article url header, then performs text cleaning
    article_header = article.url.lower() + ' ' + ' '.join(article.meta_keywords or [])
    if not any(keyword in article_header for keyword in VALID_CATEGORIES):
        print("Article is not in a valid category! Please provide a relevant article.")
        return False


    # Final return statement to indicate that the article is valid
    return True


# Function that scrapes a single article using newspaper3k library
def scrapeSingleArticle(websiteUrl):
    # Exception Handling to catch errors
    try:        
        # Validation #1: Checks if the URL is valid (no other formats)
        if not websiteUrl.startswith(('http://', 'https://')):
            print(f"Invalid URL Type! Please provide a valid URL.")
            return None, None, None, None

        # Creates an article object of the website
        article = Article(websiteUrl)

        # Downloads & parses the article
        article.download()
        article.parse()

        # Validate article using the helper function
        if not validateArticle(article):
            return None, None, None, None

        # Extracts title, then performs text cleaning
        titleTag = article.title
        if titleTag:
            title = titleTag.strip()
        else:
           title = 'Title not found.'
        
        # Extracts the date published, then performs text cleaning
        dateTag = article.publish_date
        if dateTag:
            date = dateTag.strftime('%Y-%m-%d')
        else:
            date = 'Date not found.'

        # Extracts publisher, then performs text cleaning
        publisher = article.source_url.strip()
        if publisher:
            publisher = publisher
        else:
            publisher = 'Publisher not found.'
            
        # Extracts body of the article
        body = article.text.strip()
        if body:
            body = body
        else:
            body = 'Body not found.'

        # Removes stop words from the body of the article
        bodyWords = body.split()
        filteredBody = [word for word in bodyWords if word.lower() not in stopWords]
        body = ' '.join(filteredBody)

        return title, date, publisher, body

    # Exception Case 1: Unable to retrieve article
    except newspaper.article.ArticleException:
        print(f"Provided URL [{websiteUrl}] not found.")
        return None, None, None, None

    # Exception Case 2: All other errors
    except Exception as err:
        print(f"An unexpected error occurred: {err}")
        return None, None, None, None


# Testing the scrapeSingleArticle(websiteUrl) function
url = 'https://www.bbc.com/news/world-us-canada-68790777'

title, date, publisher, body = scrapeSingleArticle(url)

print("Title:", title)
print("Publisher:", publisher)
print("Date Published:", date)
print("Body:", body)





#############################################################################################################
#############################################################################################################
#############################################################################################################
################################### SECTION 2: MODEL TRAINING RESOURCES #####################################
#############################################################################################################
#############################################################################################################
#############################################################################################################


# Using https://www.geeksforgeeks.org/creating-a-dataframe-using-excel-files/
def scrapeAllArticles(filename):
    articles = []

    # Exception Handling to catch errors
    try:
        # Reads the Excel file using the pandas library 
        dataFrame = pd.read_excel(filename, header=None)

        # Extracts URLs col 10 with the URL links using a loop
        urls = []
        for i in range(len(dataFrame)):
            urls.append(dataFrame[10][i])

        # Loops through each URL in the list
        for url in urls:
            print(f"\nURL #{urls.index(url) + 1} found from Excel: {url}")
        
            currentArticle = url
            title, date, publisher, body = scrapeSingleArticle(currentArticle)

            # Skips invalid articles (title, date, publisher, body fields with None values)
            #if not title or not date or not publisher or not body:
            #   continue

            # Appends the extracted information to the list
            articles.append({
                'title': title, 
                'date': date, 
                'publisher': publisher, 
                'body': body
            })

            # Prints for testing
            #print('Articles scraped:', articles)   
            print("title:", title)
            print("date:", date)
            print("publisher:", publisher)
            print("body:", body)
            print (f"Articles scraped: {len(articles)}")

        # Returns the list of articles scraped
        return articles

    # Exception Cases
    except FileNotFoundError:
        print("File Not Found!")
    except ValueError as e:
        print("Value Error!", str(e))
    except Exception as e:
        print("An unexpected error occurred!", str(e))


filename = 'articles.xlsm'  

# Make sure to have the xlsx file in the same directory as this file
# Close the xlsx file while running the test to avoid permissions error
scraped_articles = scrapeAllArticles(filename)
