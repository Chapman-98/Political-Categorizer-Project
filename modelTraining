# @authors: Harshitha M, Nermeen R
# Used for our backend ML training purposes

# Import necessary libraries
import newspaper
import nltk
import string
from newspaper import Article
from bs4 import BeautifulSoup
from nltk.corpus import stopwords





#############################################################################################################
#############################################################################################################
#############################################################################################################
####################################### SECTION 1: INDIVIDUAL URL ###########################################
#############################################################################################################
#############################################################################################################
#############################################################################################################


# Global list of valid categories
VALID_CATEGORIES = [
    'politics', 'news', 'election', 'government', 'policy', 'congress', 'senate', 'house of representatives', 'legislation'
] # refined search

VALID_CATEGORIES2 = [
    'abdication', 'absolute monarchy', 'allegiance', 'anarchism', 'annexation', 'aristocracy', 'autonomy',
    'balance of power', 'bicameral system', 'bipartisan government', 'bureaucracy',
    'campaign, political', 'civil liberty', 'civil service', 'colonization', 'communism', 'conservatism', 'constitution',
    'convention', 'corrupt practices', 'democracy', 'deportation', 'despotism', 'election', 'embargo', 'executive',
    'fascism', 'federal government', 'federation', 'geopolitics', 'gerrymander', 'government', 'impeachment',
    'imperialism', 'judiciary', 'legislature', 'lobbying', 'military government', 'nationalism', 'natural rights', 'parliamentary law',
    'political', 'political action committee', 'political science', 'populism', 'president', 'vice president', 
    'republic', 'republicanism', 'republican government', 'separation of powers', 'socialism', 'sovereignty','veto', 'voting'
] # extended search retreieved from https://www.infoplease.com/encyclopedia/social-science/government/concepts



# A helper function that validates the given article
def vailidateArticle(article):
    
    # Validation #2: Check if the article is not a video article by checking the text size since video articles have more text
    if (len(article.text.split()) < 100):  
        print("video articles are not acceptable.")
        return False

    # Validation #3: Check if the article is in English
    if article.meta_lang != 'en':
        print("Article not in English! Please provide an English article.")
        return False

    # Validation #4: Check if the article category is valid
    # Retrieves the article url header, then performs text cleaning
    article_header = article.url.lower() + ' ' + ' '.join(article.meta_keywords or [])
    if not any(keyword in article_header for keyword in VALID_CATEGORIES):
        print("Article is not in a valid category! Please provide a relevant article.")
        return False

    # Final return statement to indicate that the article is valid
    return True


# Function that scrapes a single article using newspaper3k library
def scrapeSingleArticle(websiteUrl):
    # Exception Handling to catch errors
    try:        
        # Validation #1: Checks if the URL is valid (no other formats)
        if not websiteUrl.startswith(('http://', 'https://')):
            print(f"Invalid URL Type! Please provide a valid URL.")
            return None, None, None, None

        # Creates an article object of the website
        article = Article(websiteUrl)

        # Downloads & parses the article
        article.download()
        article.parse()

        # Validate article using the helper function
        if not vailidateArticle(article):
            return None, None, None, None

        # Extracts title, then performs text cleaning
        titleTag = article.title
        if titleTag:
            title = titleTag.strip()
        else:
           title = 'Title not found.'
        
        # Extracts the date published, then performs text cleaning
        dateTag = article.publish_date
        if dateTag:
            date = dateTag.strftime('%Y-%m-%d')
        else:
            date = 'Date not found.'

        # Extracts publisher, then performs text cleaning
        publisher = article.source_url.strip()
        if publisher:
            publisher = publisher
        else:
            publisher = 'Publisher not found.'
            
        # Extracts body of the article
        body = article.text.strip()
        if body:
            body = body
        else:
            body = 'Body not found.'

        # Creates a set of stop words (generic words)
        nltk.download('stopwords')
        stopWords = set(stopwords.words('english'))

        # Removes stop words from the body of the article
        bodyWords = body.split()
        filteredBody = [word for word in bodyWords if word.lower() not in stopWords]
        body = ' '.join(filteredBody)

        return title, date, publisher, body

    # Exception Case 1: Unable to retrieve article
    except newspaper.article.ArticleException:
        print(f"Provided URL [{websiteUrl}] not found.")
        return None, None, None, None

    # Exception Case 2: All other errors
    except Exception as err:
        print(f"An unexpected error occurred: {err}")
        return None, None, None, None

 
# Testing the scrapeSingleArticle(websiteUrl) function
url = 'https://www.foxnews.com/us/maryland-woman-pleads-guilty-conspiracy-alleged-extremist-plot-attack-baltimore-power-grid'
url1 = 'https://www.allsides.com/story/supreme-court-supreme-courts-cfpb-ruling-highlights-disagreements-delegating-congressional'
url2 = 'https://elpais.com/mexico/elecciones-mexicanas/2024-05-19/el-ultimo-intento-de-la-oposicion-mexicana-para-concurrir-unida-contra-la-candidata-oficialista.html'
url3 = 'https://edition.cnn.com/2024/05/18/sport/usyk-fury-undisputed-champion-intl-hnk/index.html'
url4 = 'https://edition.cnn.com/2024/05/18/sport/preakness-stakes-spt-intl/index.html'
url5 = 'https://edition.cnn.com/2024/05/14/sport/lebron-james-courtside-cleveland-cavaliers-boston-celtics-spt-intl/index.html'
url6 = 'https://edition.cnn.com/2024/05/18/entertainment/saturday-night-live-season-49-best-sketches/index.html'
video = 'https://www.cnn.com/videos'

'''
title, date, publisher, body = scrapeSingleArticle(video)

print("Title:", title)
print("Publisher:", publisher)
print("Date Published:", date)
print("Body:", body)'''





#############################################################################################################
#############################################################################################################
#############################################################################################################
################################### SECTION 2: MODEL TRAINING RESOURCES #####################################
#############################################################################################################
#############################################################################################################
#############################################################################################################


# Function that scrapes all articles from a given webpage
def scrapeAllArticles(webpageUrl):
    # Creates an article object of the website
    '''
        memoize_articles=False to avoid duplicate articles 
        From: newspaper3k Docs
        By default, newspaper caches all previously extracted articles and eliminates any article which it has already extracted.
        This feature exists to prevent duplicate articles and to increase extraction speed.
    '''
    webpage = newspaper.build(webpageUrl, memoize_articles=False)  
    
    # Checks if there are any articles in the webpage
    if not webpage.articles:
        print(f"Articles not found for {webpageUrl}")
        return
    
    # List to be returned by the function
    articlesInfo = []

    # Iterates over each article in the webpage
    for article in webpage.articles:
        print(f"\nURL #{webpage.articles.index(article) + 1} found from webpage: {article.url}")
        
        # Calls the scrapeSingleArticle() function to scrape the article
        currentArticle = article.url
        title, date, publisher, body = scrapeSingleArticle(currentArticle)

        # Skips invalid articles (title, date, publisher, body fields with None values)
        if not title or not date or not publisher or not body:
            continue

        # Appends the extracted information to the list
        articlesInfo.append({
            'title': title, 
            'date': date, 
            'publisher': publisher, 
            'body': body
        })

        # Prints for testing
        # print('Articles scraped:', articlesInfo)   
        print (f"Articles scraped: {len(articlesInfo)}")
 

    return articlesInfo


# Testing the scrapeAllArticles(webpageUrl) function
webpageUrl1 = 'https://www.cnn.com/politics'
#webpageUrl2 = 'https://www.foxnews.com/politics'
scrapeAllArticles(webpageUrl1)
